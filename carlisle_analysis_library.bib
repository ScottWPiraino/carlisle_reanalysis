Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{Team2016,
author = {{R Core Team}},
title = {{R: A Language and Environment for Statistical Computing}},
year = {2016}
}
@article{Carlisle2015,
abstract = {In a previous paper, one of the authors (JBC) used a chi-squared method to analyse the means (SD) of baseline variables, such as height or weight, from randomised controlled trials by Fujii et al., concluding that the probabilities that the reported distributions arose by chance were infinitesimally small. Subsequent testing of that chi-squared method, using simulation, suggested that the method was incorrect. This paper corrects the chi-squared method and tests its performance and the performance of Monte Carlo simulations and ANOVA to analyse the probability of random sampling. The corrected chi-squared method and ANOVA method became inaccurate when applied to means that were reported imprecisely. Monte Carlo simulations confirmed that baseline data from 158 randomised controlled trials by Fujii et al. were different to those from 329 trials published by other authors and that the distribution of Fujii et al.'s data were different to the expected distribution, both p {\textless} 10(-16) . The number of Fujii randomised controlled trials with unlikely distributions was less with Monte Carlo simulation than with the 2012 chi-squared method: 102 vs 117 trials with p {\textless} 0.05; 60 vs 86 for p {\textless} 0.01; 30 vs 56 for p {\textless} 0.001; and 12 vs 24 for p {\textless} 0.00001, respectively. The Monte Carlo analysis nevertheless confirmed the original conclusion that the distribution of the data presented by Fujii et al. was extremely unlikely to have arisen from observed data. The Monte Carlo analysis may be an appropriate screening tool to check for non-random (i.e. unreliable) data in randomised controlled trials submitted to journals.},
author = {Carlisle, J. B. and Dexter, F. and Pandit, J. J. and Shafer, S. L. and Yentis, S. M.},
doi = {10.1111/anae.13126},
issn = {00032409},
journal = {Anaesthesia},
month = {jul},
number = {7},
pages = {848--858},
pmid = {26032950},
title = {{Calculating the probability of random sampling for continuous variables in submitted or published randomised controlled trials}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26032950 http://doi.wiley.com/10.1111/anae.13126},
volume = {70},
year = {2015}
}
@misc{Chihara2016,
author = {Chihara, Laura},
title = {{CarletonStats: Functions for Statistics Classes at Carleton College}},
year = {2016}
}
@article{Nuijten2016,
abstract = {This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package "statcheck." statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called "co-pilot model," and to use statcheck to flag possible inconsistencies in one's own manuscript or during the review process.},
author = {Nuijten, Mich?le B. and Hartgerink, Chris H. J. and van Assen, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
doi = {10.3758/s13428-015-0664-2},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {False positives,NHST,Publication bias,Questionable research practices,Reporting errors,Significance,p-values},
month = {dec},
number = {4},
pages = {1205--1226},
pmid = {26497820},
title = {{The prevalence of statistical reporting errors in psychology (1985-2013)}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26497820 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5101263 http://link.springer.com/10.3758/s13428-015-0664-2},
volume = {48},
year = {2016}
}
@article{Keilwagen2014,
author = {Keilwagen, Jens and Grosse, Ivo and Grau, Jan and Haldemann, B and Posch, S},
doi = {10.1371/journal.pone.0092209},
editor = {Chen, Zhongxue},
issn = {1932-6203},
journal = {PLoS ONE},
month = {mar},
number = {3},
pages = {e92209},
publisher = {Public Library of Science},
title = {{Area under Precision-Recall Curves for Weighted and Unweighted Data}},
url = {http://dx.plos.org/10.1371/journal.pone.0092209},
volume = {9},
year = {2014}
}
@article{Carlisle2017,
abstract = {A large number of randomised trials authored by Yoshitaka Fujii have been retracted, in part as a consequence of a previous analysis finding a very low probability of random sampling. Dr Yuhji Saitoh co-authored 34 of those trials and he was corresponding author for eight of them. We found a number of additional randomised, controlled trials that included baseline data, with Saitoh as corresponding author, that Fujii did not co-author. We used Monte Carlo simulations to analyse the baseline data from 32 relevant trials in total as well as an outcome (muscle twitch recovery ratios) reported in several. We also compared a series of muscle twitch recovery graphs appearing in a number of Saitoh's publications. The baseline data in 14/32 randomised, controlled trials had p {\textless} 0.01, of which seven p values were {\textless} 0.001. Eight trials reported four ratios of the time for the return of muscle activity after neuromuscular blockade, the distributions of which were homogeneous: the p values for the observed Q statistics were 0.0055, 0.031, 0.016 and 0.0071. Comparison of graphs revealed multiple coincident or near-coincident curves across a large number of publications, a finding also inconsistent with random sampling. Combining the continuous and categorical probabilities of the 32 included trials, we found a very low likelihood of random sampling: p = 1.27 × 10(-8) (1 in 100,000,000). The high probability of non-random sampling and the repetition of lines in multiple graphs suggest that further scrutiny of Saitoh's work is warranted.},
author = {Carlisle, J. B. and Loadsman, J. A.},
doi = {10.1111/anae.13650},
issn = {00032409},
journal = {Anaesthesia},
keywords = {controlled trials,data fabrication,fraud,randomised},
month = {jan},
number = {1},
pages = {17--27},
pmid = {27988952},
title = {{Evidence for non-random sampling in randomised, controlled trials by Yuhji Saitoh}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27988952 http://doi.wiley.com/10.1111/anae.13650},
volume = {72},
year = {2017}
}
@article{Baker2016,
author = {Baker, Monya},
doi = {10.1038/540151a},
issn = {0028-0836},
journal = {Nature},
month = {nov},
number = {7631},
pages = {151--152},
title = {{Stat-checking software stirs up psychology}},
url = {http://www.nature.com/doifinder/10.1038/540151a},
volume = {540},
year = {2016}
}
@article{Anaya2016,
abstract = {GRIMMER (Granularity-Related Inconsistency of Means Mapped to Error Repeats) builds upon the GRIM test and allows for testing whether reported measures of variability are mathematically possible. GRIMMER relies upon the statistical phenomenon that variances display a simple repetitive pattern when the data is discrete, i.e. granular. This observation allows for the generation of an algorithm that can quickly identify whether a reported statistic of any size or precision is consistent with the stated sample size and granularity. My implementation of the test is available at PrePubMed (http://www.prepubmed.org/grimmer) and currently allows for testing variances, standard deviations, and standard errors for integer data. It is possible to extend the test to other measures of variability such as deviation from the mean, or apply the test to non-integer data such as data reported to halves or tenths. The ability of the test to identify inconsistent statistics relies upon four factors: (1) the sample size; (2) the granularity of the data; (3) the precision (number of decimals) of the reported statistic; and (4) the size of the standard deviation or standard error (but not the variance). The test is most powerful when the sample size is small, the granularity is large, the statistic is reported to a large number of decimal places, and the standard deviation or standard error is small (variance is immune to size considerations). This test has important implications for any field that routinely reports statistics for granular data to at least two decimal places because it can help identify errors in publications, and should be used by journals during their initial screen of new submissions. The errors detected can be the result of anything from something as innocent as a typo or rounding error to large statistical mistakes or unfortunately even fraud. In this report I describe the mathematical foundations of the GRIMMER test and the algorithm I use to implement it.},
author = {Anaya, Jordan},
doi = {10.7287/peerj.preprints.2400v1},
issn = {2167-9843},
keywords = {replicability,reproducibility,standard deviations,standard errors,statistics,variances},
month = {jan},
publisher = {PeerJ Inc.},
title = {{The GRIMMER test: A method for testing the validity of reported measures of variability}},
url = {https://peerj.com/preprints/2400/},
year = {2016}
}
@article{Fanelli2009,
author = {Fanelli, Daniele and Kerridge, IH and Hill, SR and McNeill, PM and Doran, E},
doi = {10.1371/journal.pone.0005738},
editor = {Tregenza, Tom},
issn = {1932-6203},
journal = {PLoS ONE},
month = {may},
number = {5},
pages = {e5738},
publisher = {Southern Illinois University.Doctoral dissertation},
title = {{How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data}},
url = {http://dx.plos.org/10.1371/journal.pone.0005738},
volume = {4},
year = {2009}
}
@misc{Dewey2017,
author = {Dewey, Michael},
title = {metap: meta-analysis of significance values},
year = {2017}
}
@article{Simmons2011,
abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
issn = {1467-9280},
journal = {Psychological science},
month = {nov},
number = {11},
pages = {1359--66},
pmid = {22006061},
title = {{False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant.}},
url = {http://journals.sagepub.com/doi/10.1177/0956797611417632 http://www.ncbi.nlm.nih.gov/pubmed/22006061},
volume = {22},
year = {2011}
}
@article{Al-Marzouki2005,
abstract = {OBJECTIVES To test the application of statistical methods to detect data fabrication in a clinical trial. SETTING Data from two clinical trials: a trial of a dietary intervention for cardiovascular disease and a trial of a drug intervention for the same problem. OUTCOME MEASURES Baseline comparisons of means and variances of cardiovascular risk factors; digit preference overall and its pattern by group. RESULTS In the dietary intervention trial, variances for 16 of the 22 variables available at baseline were significantly different, and 10 significant differences were seen in means for these variables. Some of these P values were extraordinarily small. Distributions of the final recorded digit were significantly different between the intervention and the control group at baseline for 14/22 variables in the dietary trial. In the drug trial, only five variables were available, and no significant differences between the groups for baseline values in means or variances or digit preference were seen. CONCLUSIONS Several statistical features of the data from the dietary trial are so strongly suggestive of data fabrication that no other explanation is likely.},
author = {Al-Marzouki, Sanaa and Evans, Stephen and Marshall, Tom and Roberts, Ian},
doi = {10.1136/bmj.331.7511.267},
issn = {1756-1833},
journal = {BMJ (Clinical research ed.)},
keywords = {Biomedical and Behavioral Research,Empirical Approach},
month = {jul},
number = {7511},
pages = {267--70},
pmid = {16052019},
title = {{Are these data real? Statistical methods for the detection of data fabrication in clinical trials.}},
url = {http://www.bmj.com/cgi/doi/10.1136/bmj.331.7511.267 http://www.ncbi.nlm.nih.gov/pubmed/16052019 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1181267},
volume = {331},
year = {2005}
}
@misc{Bryan2017,
author = {Wickham, Hadley and Bryan, Jennifer},
title = {{readxl: Read Excel Files}},
year = {2017}
}
@article{Loadsman2017,
author = {Loadsman, J A and McCulloch, T J},
doi = {10.1111/anae.13962},
issn = {1365-2044},
journal = {Anaesthesia},
keywords = {controlled trials,data error,fraud,randomised},
month = {jun},
pmid = {28580657},
title = {{Widening the search for suspect data - is the flood of retractions about to become a tsunami?}},
url = {http://doi.wiley.com/10.1111/anae.13962 http://www.ncbi.nlm.nih.gov/pubmed/28580657},
year = {2017}
}
@article{Schmidt2016,
abstract = {STATCHECK is an R algorithm designed to scan papers automatically for inconsistencies between test statistics and their associated p values (Nuijten et al., 2016). The goal of this comment is to point out an important and well-documented flaw in this busily applied algorithm: It cannot handle corrected p values. As a result, statistical tests applying appropriate corrections to the p value (e.g., for multiple tests, post-hoc tests, violations of assumptions, etc.) are likely to be flagged as reporting inconsistent statistics, whereas papers omitting necessary corrections are certified as correct. The STATCHECK algorithm is thus valid for only a subset of scientific papers, and conclusions about the quality or integrity of statistical reports should never be based solely on this program.},
archivePrefix = {arXiv},
arxivId = {1610.01010},
author = {Schmidt, Thomas},
eprint = {1610.01010},
month = {oct},
title = {{Sources of false positives and false negatives in the STATCHECK algorithm: Reply to Nuijten et al. (2016)}},
url = {http://arxiv.org/abs/1610.01010},
year = {2016}
}
@article{OpenScienceCollaboration2015,
abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
issn = {0036-8075},
journal = {Science},
month = {aug},
number = {6251},
pages = {aac4716--aac4716},
pmid = {26315443},
title = {{Estimating the reproducibility of psychological science}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26315443 http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@misc{Wickham2009,
author = {Wickham, Hadley},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
year = {2009}
}
@article{Carlisle2012,
abstract = {The population sampling in randomised controlled trials by Fujii et al. have been shown to exhibit unusual distributions. This systematic review analysed the effectiveness of prophylactic antiemetics in trials by Fujii et al. compared with other authors. Granisetron was more effective in trials by Fujii et al., relative risk ratios (RRR (95{\%} CI)): nausea 0.53 (0.42-0.67), p=0.00021; vomiting 0.60 (0.50-0.73), p=0.00094. Ramosetron was also more effective in studies by Fujii et al.: vomiting 0.60 (0.39-0.91), p=0.02; nausea or vomiting 0.71 (0.56-0.91); p=0.006. In comparison with granisetron, droperidol was less effective in trials by Fujii et al. than others: nausea 2.41 (1.72-3.36), p=2.5×10(-7); vomiting 1.73 (1.26-2.38), p=6.4×10(-4). Postoperative nausea and vomiting was less likely to trigger rescue antiemesis after granisetron and metoclopramide in studies by Fujii et al., 0.40 (0.27-0.60), p=9.7×10(-6). Triggered rates of rescue were not different in studies by others for droperidol, granisetron and metoclopramide, but were less common after granisetron than droperidol and metoclopramide in studies by Fujii et al., 0.50 (0.38-0.66), p=1.7×10(-6) and 0.47 (0.34-0.64), p=2.6×10(-6), respectively. There was no synergism between antiemetics in trials by other authors. In contrast, in studies by Fujii et al., postoperative nausea and vomiting was more likely if granisetron was administered alone: nausea 4.20 (1.94-9.08), p=2.6×10(-4) ; vomiting 4.50 (2.55-7.97), p=2.3×10(-7); nausea or vomiting 5.00 (2.84-8.81), p=2.5×10(-8). Similarly, droperidol was less effective in studies by Fujii et al. if administered alone: vomiting 2.76 (1.25-6.11), p=0.01; nausea or vomiting 2.96 (1.46-6.00), p=2.7×10(-3). The conclusion is that if, as recommended, data with unusual distributions are removed from meta-analysis and articles by Fujii et al. excluded, then the antiemetic effects of granisetron and ramosetron are greatly reduced; further, there is no evidence of synergism between antiemetics and indeed, some evidence of antagonism between antiemetic agents.},
author = {Carlisle, J. B.},
doi = {10.1111/j.1365-2044.2012.07232.x},
issn = {00032409},
journal = {Anaesthesia},
month = {oct},
number = {10},
pages = {1076--1090},
pmid = {22734848},
title = {{A meta-analysis of prevention of postoperative nausea and vomiting: randomised controlled trials by Fujii et al. compared with other authors}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22734848 http://doi.wiley.com/10.1111/j.1365-2044.2012.07232.x},
volume = {67},
year = {2012}
}
@article{Chang2015,
abstract = {We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication files that include both data and code. Some journals in our sample require data and code replication files, and other journals do not require such files. Aside from 6 papers that use confidential data, we obtain data and code replication files for 29 of 35 papers (83{\%}) that are required to provide such files as a condition of publication, compared to 11 of 26 papers (42{\%}) that are not required to provide data and code replication files. We successfully replicate the key qualitative result of 22 of 67 papers (33{\%}) without contacting the authors. Excluding the 6 papers that use confidential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49{\%}) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.},
author = {Chang, Andrew C and Li, Phillip and Hanson, Tyler J and Larsson, Erik and Mai, Kim T and Marcozzi, Anthony and Martin, Shawn M and Radler, Tyler},
doi = {10.17016/FEDS.2015.083},
journal = { Finance and Economics Discussion Series 2015-083 Washington: Board of Governors of the Federal Reserve System},
number = {Series 2015-083},
title = {{Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say ”Usually Not”}},
url = {http://ssrn.com/abstract=2669564},
year = {2015}
}
@article{Carlisle2017a,
abstract = {Randomised, controlled trials have been retracted after publication because of data fabrication and inadequate ethical approval. Fabricated data have included baseline variables, for instance, age, height or weight. Statistical tests can determine the probability of the distribution of means, given their standard deviation and the number of participants in each group. Randomised, controlled trials have been retracted after the data distributions have been calculated as improbable. Most retracted trials have been written by anaesthetists and published by specialist anaesthetic journals. I wanted to explore whether the distribution of baseline data in trials was consistent with the expected distribution. I wanted to determine whether trials retracted after publication had distributions different to trials that have not been retracted. I wanted to determine whether data distributions in trials published in specialist anaesthetic journals have been different to distributions in non-specialist medical journals. I analysed the distribution of 72,261 means of 29,789 variables in 5087 randomised, controlled trials published in eight journals between January 2000 and December 2015: Anaesthesia (399); Anesthesia and Analgesia (1288); Anesthesiology (541); British Journal of Anaesthesia (618); Canadian Journal of Anesthesia (384); European Journal of Anaesthesiology (404); Journal of the American Medical Association (518) and New England Journal of Medicine (935). I chose these journals as I had electronic access to the full text. Trial p values were distorted by an excess of baseline means that were similar and an excess that were dissimilar: 763/5015 (15.2{\%}) trials that had not been retracted from publication had p values that were within 0.05 of 0 or 1 (expected 10{\%}), that is, a 5.2{\%} excess, p = 1.2 × 10(-7) . The p values of 31/72 (43{\%}) trials that had been retracted after publication were within 0.05 of 0 or 1, a rate different to that for unretracted trials, p = 1.03 × 10(-10) . The difference between the distributions of these two subgroups was confirmed by comparison of their overall distributions, p = 5.3 × 10(-15) . Each journal exhibited the same abnormal distribution of baseline means. There was no difference in distributions of baseline means for 1453 trials in non-anaesthetic journals and 3634 trials in anaesthetic journals, p = 0.30. The rate of retractions from JAMA and NEJM, 6/1453 or 1 in 242, was one-quarter the rate from the six anaesthetic journals, 66/3634 or 1 in 55, relative risk (99{\%}CI) 0.23 (0.08-0.68), p = 0.00022. A probability threshold of 1 in 10,000 identified 8/72 (11{\%}) retracted trials (7 by Fujii et al.) and 82/5015 (1.6{\%}) unretracted trials. Some p values were so extreme that the baseline data could not be correct: for instance, for 43/5015 unretracted trials the probability was less than 1 in 10(15) (equivalent to one drop of water in 20,000 Olympic-sized swimming pools). A probability threshold of 1 in 100 for two or more trials by the same author identified three authors of retracted trials (Boldt, Fujii and Reuben) and 21 first or corresponding authors of 65 unretracted trials. Fraud, unintentional error, correlation, stratified allocation and poor methodology might have contributed to the excess of randomised, controlled trials with similar or dissimilar means, a pattern that was common to all the surveyed journals. It is likely that this work will lead to the identification, correction and retraction of hitherto unretracted randomised, controlled trials.},
author = {Carlisle, J. B.},
doi = {10.1111/anae.13938},
issn = {00032409},
journal = {Anaesthesia},
keywords = {data error,fraud,randomised, controlled trials},
month = {jun},
pmid = {28580651},
title = {{Data fabrication and other reasons for non-random sampling in 5087 randomised, controlled trials in anaesthetic and general medical journals}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28580651 http://doi.wiley.com/10.1111/anae.13938},
year = {2017}
}
@article{Casadevall2014,
abstract = {Retraction of flawed articles is an important mechanism for correction of the scientific literature. We recently reported that the majority of retractions are associated with scientific misconduct. In the current study, we focused on the subset of retractions for which no misconduct was identified, in order to identify the major causes of error. Analysis of the retraction notices for 423 articles indexed in PubMed revealed that the most common causes of error-related retraction are laboratory errors, analytical errors, and irreproducible results. The most common laboratory errors are contamination and problems relating to molecular biology procedures (e.g., sequencing, cloning). Retractions due to contamination were more common in the past, whereas analytical errors are now increasing in frequency. A number of publications that have not been retracted despite being shown to contain significant errors suggest that barriers to retraction may impede correction of the literature. In particular, few cases of retraction due to cell line contamination were found despite recognition that this problem has affected numerous publications. An understanding of the errors leading to retraction can guide practices to improve laboratory research and the integrity of the scientific literature. Perhaps most important, our analysis has identified major problems in the mechanisms used to rectify the scientific literature and suggests a need for action by the scientific community to adopt protocols that ensure the integrity of the publication process.},
author = {Casadevall, A. and Steen, R. G. and Fang, F. C.},
doi = {10.1096/fj.14-256735},
issn = {0892-6638},
journal = {The FASEB Journal},
keywords = {bibliometric analysis,biomedical publishing,ethics},
month = {sep},
number = {9},
pages = {3847--3855},
pmid = {24928194},
title = {{Sources of error in the retracted scientific literature}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24928194 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5395722 http://www.fasebj.org/cgi/doi/10.1096/fj.14-256735},
volume = {28},
year = {2014}
}
@misc{Buranyi2017a,
author = {Buranyi, Stephen and Devlin, Hannah},
booktitle = {The Guardian},
title = {{Dozens of recent clinical trials may contain wrong or falsified data, claims study}},
urldate = {2017-07-14},
year = {2017}
}
@article{Brown2016,
abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20{\%} (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
author = {Brown, Nicholas J. L. and Heathers, James A. J.},
doi = {10.1177/1948550616673876},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {advanced quantitative methods,philosophy of science,research methods},
month = {oct},
pages = {194855061667387},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{The GRIM Test}},
url = {http://journals.sagepub.com/doi/10.1177/1948550616673876},
year = {2016}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P. A.},
doi = {10.1371/journal.pmed.0020124},
issn = {1549-1676},
journal = {PLoS Medicine},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why Most Published Research Findings Are False}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16060722 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1182327 http://dx.plos.org/10.1371/journal.pmed.0020124},
volume = {2},
year = {2005}
}
@article{Darlington2000,
abstract = {The Stouffer z method, and other popular methods for combining p values from independent significance tests, suffer from three problems: vulnerability to criti-cisms of the individual studies being pooled, difficulty in handling the "file drawer problem," and vague conclusions. These problems can be reduced or eliminated by supplementing a test of combined probability with a variety of new analyses de-scribed here. Along with other advantages, these analyses provide a way to address the file drawer problem without making limiting assumptions about the nature of studies not included in the pooled analysis. These analyses can supplement a traditional meta-analysis, yielding conclusions not provided by widely used meta-analytic procedures. Today it is rare that a literature search reveals only one study on a topic of interest. Rather, there may be several, or dozens, or even hundreds of studies on a single topic. This fact has led to the rise of meta-analysis, which has revolutionized the way we think about literature reviews. The abundance of studies has also led to the widespread use of tests of combined significance. These tests are often discussed in gen-eral works on meta-analysis such as Becker (1994), Glass, McGaw, and Smith (1981), or Rosenthal (1991), but they represent a distinct offshoot from the main branch of meta-analysis, which focuses on effect size. Tests of combined significance are also called probability poolers. The purpose of probability pool-ers is to show that a positive effect exists in at least some of the studies under analysis, without asking about the size of typical effects or the factors affecting effect size.},
author = {Darlington, Richard B and Hayes, Andrew F and Darlington, Richard},
journal = {Psychological Methods},
number = {4},
pages = {496--515},
title = {{Combining Independent p Values: Extensions of the Stouffer and Binomial Methods}},
url = {https://pdfs.semanticscholar.org/5f0b/8e577027d1e1a411c4da9e2fd5a9892895e1.pdf},
volume = {5},
year = {2000}
}
@article{Bolland2016,
abstract = {BACKGROUND Statistical techniques can investigate data integrity in randomized controlled trials (RCTs). We systematically reviewed and analyzed all human RCTs undertaken by a group of researchers, about which concerns have been raised. METHODS We compared observed distributions of p values for between-groups differences in baseline variables, for standardized sample means for continuous baseline variables, and for differences in treatment group participant numbers with the expected distributions. We assessed productivity, recruitment rates, outcome data, textual consistency, and ethical oversight. RESULTS The researchers were remarkably productive, publishing 33 RCTs over 15 years involving large numbers of older patients with substantial comorbidity, recruited over very short periods. Treatment groups were improbably similar. The distribution of p values for differences in baseline characteristics differed markedly from the expected uniform distribution (p = 5.2 × 10(-82)). The distribution of standardized sample means for baseline continuous variables and the differences between participant numbers in randomized groups also differed markedly from the expected distributions (p = 4.3 × 10(-4), p = 1.5 × 10(-5), respectively). Outcomes were remarkably positive, with very low mortality and study withdrawals despite substantial comorbidity. There were very large reductions in hip fracture incidence, regardless of intervention (relative risk 0.22, 95{\%} confidence interval 0.15-0.31, p {\textless} 0.0001, range of relative risk 0.10-0.33), that greatly exceed those reported in meta-analyses of other trials. There were multiple examples of inconsistencies between and within trials, errors in reported data, misleading text, duplicated data and text, and uncertainties about ethical oversight. CONCLUSIONS A systematic approach using statistical techniques to assess randomization outcomes can evaluate data integrity, in this case suggesting these RCT results may be unreliable.},
author = {Bolland, Mark J. and Avenell, Alison and Gamble, Greg D. and Grey, Andrew},
doi = {10.1212/WNL.0000000000003387},
issn = {0028-3878},
journal = {Neurology},
month = {dec},
number = {23},
pages = {2391--2402},
pmid = {27920281},
title = {{Systematic review and statistical analysis of the integrity of 33 randomized controlled trials}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27920281 http://www.neurology.org/lookup/doi/10.1212/WNL.0000000000003387},
volume = {87},
year = {2016}
}
@article{Simonsohn2013,
abstract = {I argue that requiring authors to post the raw data supporting their published results has the benefit, among many others, of making fraud much less likely to go undetected. I illustrate this point by describing two cases of suspected fraud I identified exclusively through statistical analysis of reported means and standard deviations. Analyses of the raw data behind these published results provided invaluable confirmation of the initial suspicions, ruling out benign explanations (e.g., reporting errors, unusual distributions), identifying additional signs of fabrication, and also ruling out one of the suspected fraud's explanations for his anomalous results. If journals, granting agencies, universities, or other entities overseeing research promoted or required data posting, it seems inevitable that fraud would be reduced.},
author = {Simonsohn, Uri},
doi = {10.1177/0956797613480366},
issn = {1467-9280},
journal = {Psychological science},
keywords = {data posting,data sharing,decision making,fake data,judgment,scientific communication},
month = {oct},
number = {10},
pages = {1875--88},
pmid = {23982243},
title = {{Just post it: the lesson from two cases of fabricated data detected by statistics alone.}},
url = {http://journals.sagepub.com/doi/10.1177/0956797613480366 http://www.ncbi.nlm.nih.gov/pubmed/23982243},
volume = {24},
year = {2013}
}
@misc{Oransky2017,
author = {Oransky, Ivan},
booktitle = {Retraction Watch},
title = {{Tracking retractions as a window into the scientific process Two in 100 clinical trials in eight major journals likely contain inaccurate data: Study}},
url = {http://retractionwatch.com/2017/06/05/two-100-clinical-trials-eight-major-journals-likely-contain-inaccurate-data-study/},
urldate = {2017-07-12},
year = {2017}
}
@misc{Buranyi2017,
author = {Buranyi, Stephen},
booktitle = {The Guardian},
title = {{The hi-tech war on science fraud}},
url = {https://www.theguardian.com/science/2017/feb/01/high-tech-war-on-science},
urldate = {2017-07-14},
year = {2017}
}
@misc{Arnholt2012,
author = {Arnholt, Alan T.},
title = {{BSDA: Basic Statistics and Data Analysis}},
year = {2012}
}
